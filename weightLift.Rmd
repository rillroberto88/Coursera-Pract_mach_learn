---
title: "Weight Lifting analysis"
author: "Robert Rill"
date: "Wednesday, June 18, 2014"
output: html_document
---

<h2>About the dataset</h2>

<div style="width:600px;text-align: justify;">
<p>
The Weight Lifting Exercises Dataset was collected with the purpose of research in the 
field of human activity recognition. Accelerometers were attached to the belt, forearm, 
arm and dumbell of 6 participants, who performed barbell lifts correctly and 
incorrectly, in 5 different ways totally. The data recorded by the devices was saved, 
and the purpose is to predict from the measurements how well were the exercises performed. 
<br/>Every measurement is marked with one of the 5 classes: A corresponds to the 
correct execution of the exercise, while B,C,D,E correspond to common mistakes. 
For more details see <a href="http://groupware.les.inf.puc-rio.br/har">this</a> webpage. 
The dataset is of importance in investigating "how well" an exercise is performed, 
an aspect that gained little attention so far, although it could provide useful 
information in many applications.
</p><hr/>

<h2>Data given and goal of the project</h2>
<p>
We were provided a training and test dataset, consisting of 19622 and 20 entries respectively, 
having 157 features and an outcome in the case of the training set, the class variable 
corresponding to the type of the exercise (A,B,C,D,E). Our task is to build a 
prediction model based on the training data, with the help of which we would be able to 
tell about a measurement, which class it belongs to. Finally, using the built 
machine learning algorithm, we had to predict the class for the test data.
</p><hr/>

<h2>Preprocessing the data</h2>
<p>
Taking a first look at the data-table, we one can easily see, that a lot of 
entries have the value NA and also many values are empty. Because of this what 
I did is that I determined those columns, where more than 95% of the data 
is not empty and not NA, and deleted the rest, keeping this way only the 
relevant columns (the ones where valid data is provided for all samples).
<br/>
Secondly from the remaining columns I eliminated the first 7 
(user index, user name, 3 timestamps, 2 window related columns), 
because those are not directly related to the measurements of the accelerometers 
attached to the participants (these features don't have to do anything with the 
values retreived by the devices). This results in a total of 53 variables, out of 
which 52 are the predictors and one is the outcome (classe):
```{r columns, echo=FALSE}
load("ColumnsToSelect")
columns
```
</p><hr/>

<h2>Model training</h2>
<p>
After preprocessing the data as described above, we end up with a set of 19622 
observations and 53 variables. As a next step I partitioned the dataset into 
training (60%) and testing (40%) sets, based on the classe variable. This way I 
ended up with the following sizes for each of these sets:
```{r, echo=FALSE}
training = read.csv("trainingDataBig.csv")
training = training[,c(-1)]
testing = read.csv("testingDataBig.csv")
testing = testing[,c(-1)]
```

```{r dims}
dim(training)
dim(testing)
```
Now, because the training set is quite big, for the purpose of visualisation I used only a smaller part of it and tried to see how the different variables act against each other. For example 
by plotting the 'total_accel_belt' versus the 'total_accel_arm' I got the following figure:
```{r, echo=FALSE, include=FALSE}
library(caret)
ind = createDataPartition(y=training$classe, p=0.2, list=FALSE)
trainingSmall = training[ind,]
load("weightModelRpart.rda");
```
```{r qplot1}
qplot(total_accel_belt,total_accel_arm,colour=classe,data=trainingSmall)
```
Looking at the figure we can see 2 clusters, but both contain all the classes. Maybe 
we could conclude for example, that if the 'total_accel_belt' is greater than 21, 
than all the samples belong to class E, but this is only a small subset, 
we can't make more important statements. Another example, if I plot the 'accel_belt_y' 
versus 'roll_dumbbell', I could again separate small subsets of the same class, but most 
of the point cannot be separated:
```{r qplot2}
qplot(accel_belt_y,roll_dumbbell,colour=classe,data=trainingSmall)
```
So my conclusion is, that all the 52 variables remaining after the preprocessing step, 
are the same importance, because plotting them in pairs, we cannot find very discinct  clusters. This is proven also by fitting a simple tree model on the hole data, with the 
'rpart' method from caret. This gives a poor result even for the training set, 
reaching only around 50% accuracy.
```{r trainPredict, include=FALSE}
preds = predict(modfitR,newdata=training)
cm = confusionMatrix(preds,training$classe)
cm$overall[1]
```

To increase accuracy, I trained a random forest model on the training data. 
I used cross validation for training control with 4 folds:
```
trControl = trainControl(method = "cv", number = 4)
```
Fitting a random forest model to the training data takes a longer time, but 
can get higher accuracy than 'rpart'. Printing out the confusion matrix of the 
final model after the training, proves my expectations. Indeed it missclassifies 
only a few samples out of the large number present in the training set:
```{r, echo = FALSE}
load("weightModelBig.rda")
modelRF = modfit
```
</p>
```{r model}
modelRF$finalModel$confusion
```
We can also see from the model, that the OOB error estimate is less than 1%. 
Predicting the outcome for the previously unseen testing data gives my estimate of the out of sample error:
```{r testPredict, include=FALSE}
predictions = predict(modelRF, newdata=testing)
confusionMatrix(predictions, testing$classe)
```
Based on the above predictions the out of sample error is very small, less than 0.8%:
```{r outOfSample}
sum(predictions!=testing$classe)/dim(testing)[1]
```

<h2>Conclusions</h2>

Based on the above reasoning and result a random forest model with cross validation 
fits well the Weight Lifting Dataset and is able to predict the class of the exercises 
for new datasets with a really high accuracy of 99% and the out of sample error beeing 
less than 1%.
<hr/>
</div>
